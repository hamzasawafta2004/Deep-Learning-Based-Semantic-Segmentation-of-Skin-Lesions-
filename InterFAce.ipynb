{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14292297,"sourceType":"datasetVersion","datasetId":9123143},{"sourceId":14309246,"sourceType":"datasetVersion","datasetId":9134806},{"sourceId":14322066,"sourceType":"datasetVersion","datasetId":9142940}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0075f16c-503b-4b22-8f64-5e71750c9caa","cell_type":"markdown","source":"# Imports\n","metadata":{}},{"id":"60792e4f-6f06-48a9-b3cc-28aca1665b18","cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.models.segmentation as segmodels\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:26:49.031872Z","iopub.execute_input":"2025-12-31T12:26:49.032146Z","iopub.status.idle":"2025-12-31T12:27:03.780676Z","shell.execute_reply.started":"2025-12-31T12:26:49.032123Z","shell.execute_reply":"2025-12-31T12:27:03.780081Z"}},"outputs":[],"execution_count":1},{"id":"fe3c9f9e-217f-40eb-87c4-94344f99bd42","cell_type":"markdown","source":"# PATHS","metadata":{}},{"id":"24a1a003-32f1-42b7-97a2-5c985e38ab30","cell_type":"code","source":"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 256\nTHRESH = 0.5\n\n\nWEIGHTS = {\n    \"VGG16_UNet\":          \"/kaggle/input/wht543/VGG16_UNet_BEST.pth\",\n    \"DeepLabV3_R50\":       \"/kaggle/input/wht543/DeepLabV3_R50_BEST.pth\",\n    \"FCN_R50\":             \"/kaggle/input/wht543/FCN_R50_fold1_dice0.8988.pth\",\n    \"ResNet34_SE_UNet\":    \"/kaggle/input/wht543/ResNet34_SE_UNet_LR3e-05_B16_best_val0.9014.pth\",\n    \"SegNet\":           \"/kaggle/input/wht543/SegNet_fold3_dice0.8915.pth\",\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.782033Z","iopub.execute_input":"2025-12-31T12:27:03.782400Z","iopub.status.idle":"2025-12-31T12:27:03.866127Z","shell.execute_reply.started":"2025-12-31T12:27:03.782367Z","shell.execute_reply":"2025-12-31T12:27:03.865357Z"}},"outputs":[],"execution_count":2},{"id":"80819ce3-38e4-43d8-9da7-8002f45d1d27","cell_type":"markdown","source":"# HELPERS","metadata":{}},{"id":"4b193bed-4fd7-4561-93fd-892af33d1c2b","cell_type":"code","source":"def preprocess_image(rgb_img, size=256):\n    \n    img = cv2.resize(rgb_img, (size, size), interpolation=cv2.INTER_AREA)\n    img = img.astype(np.float32) / 255.0\n    \n    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n    img = (img - mean) / std\n    img = np.transpose(img, (2, 0, 1))  # CHW\n    x = torch.from_numpy(img).unsqueeze(0)  # 1CHW\n    return x\n\ndef postprocess_mask(logits, out_h, out_w, thresh=0.5):\n    \n    prob = torch.sigmoid(logits)[0, 0].detach().cpu().numpy()\n    pred = (prob > thresh).astype(np.uint8) * 255\n    pred = cv2.resize(pred, (out_w, out_h), interpolation=cv2.INTER_NEAREST)\n    return pred\n\ndef overlay_mask(rgb_img, mask_u8, alpha=0.45):\n    \n    overlay = rgb_img.copy()\n    red = np.zeros_like(rgb_img)\n    red[..., 0] = 255  # red channel in RGB\n\n    m = (mask_u8 > 0).astype(np.float32)[..., None]\n    overlay = (overlay * (1 - alpha * m) + red * (alpha * m)).astype(np.uint8)\n    return overlay\n\ndef model_forward(model, x, arch_name):\n    \n    if arch_name in [\"DeepLabV3_R50\", \"FCN_R50\"]:\n        return model(x)[\"out\"]\n    return model(x)\n\ndef load_state_safely(model, weight_path):\n    if not os.path.isfile(weight_path):\n        raise FileNotFoundError(f\"Weight file not found: {weight_path}\")\n    sd = torch.load(weight_path, map_location=DEVICE)\n    model.load_state_dict(sd)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.867115Z","iopub.execute_input":"2025-12-31T12:27:03.867437Z","iopub.status.idle":"2025-12-31T12:27:03.882905Z","shell.execute_reply.started":"2025-12-31T12:27:03.867411Z","shell.execute_reply":"2025-12-31T12:27:03.882096Z"}},"outputs":[],"execution_count":3},{"id":"3bf7df71-7911-4ed5-9d46-9bca4bde8c48","cell_type":"markdown","source":"# MODEL DEFINITIONS","metadata":{}},{"id":"9a63c7c9-b48c-4712-916a-477f8a7409e4","cell_type":"code","source":"\nclass VGG16_UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n        self.enc1 = vgg[:5]\n        self.enc2 = vgg[5:10]\n        self.enc3 = vgg[10:17]\n        self.enc4 = vgg[17:24]\n        self.enc5 = vgg[24:31]\n        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n\n        def conv(i, o):\n            return nn.Sequential(\n                nn.Conv2d(i, o, 3, padding=1), nn.ReLU(inplace=True),\n                nn.Conv2d(o, o, 3, padding=1), nn.ReLU(inplace=True)\n            )\n\n        self.d5 = conv(512 + 512, 512)\n        self.d4 = conv(512 + 256, 256)\n        self.d3 = conv(256 + 128, 128)\n        self.d2 = conv(128 + 64,   64)\n        self.d1 = conv(64, 64)\n        self.final = nn.Conv2d(64, 1, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n        e3 = self.enc3(e2)\n        e4 = self.enc4(e3)\n        e5 = self.enc5(e4)\n\n        d5 = self.d5(torch.cat([self.up(e5), e4], dim=1))\n        d4 = self.d4(torch.cat([self.up(d5), e3], dim=1))\n        d3 = self.d3(torch.cat([self.up(d4), e2], dim=1))\n        d2 = self.d2(torch.cat([self.up(d3), e1], dim=1))\n        d1 = self.d1(self.up(d2))\n        return self.final(d1)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.884009Z","iopub.execute_input":"2025-12-31T12:27:03.884260Z","iopub.status.idle":"2025-12-31T12:27:03.897631Z","shell.execute_reply.started":"2025-12-31T12:27:03.884239Z","shell.execute_reply":"2025-12-31T12:27:03.896844Z"}},"outputs":[],"execution_count":4},{"id":"0c09e057-ef06-4fe7-b58c-8e0e5c64367e","cell_type":"code","source":"\nclass ConvBlock(nn.Module):\n    def __init__(self, i, o):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(i, o, 3, 1, 1), nn.ReLU(True),\n            nn.Conv2d(o, o, 3, 1, 1), nn.ReLU(True),\n        )\n    def forward(self, x): return self.net(x)\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.fc1 = nn.Linear(channel, channel // reduction, bias=False)\n        self.fc2 = nn.Linear(channel // reduction, channel, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = F.adaptive_avg_pool2d(x, (1, 1)).view(b, c)\n        y = self.fc1(y)\n        y = F.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\nclass ResNet34_SE_UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        r = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n\n        self.e0 = nn.Sequential(r.conv1, r.bn1, r.relu)  \n        self.e1 = nn.Sequential(r.maxpool, r.layer1)     \n        self.e2 = r.layer2                               \n        self.e3 = r.layer3                               \n        self.e4 = r.layer4                               \n\n        self.se1 = SEBlock(64)\n        self.se2 = SEBlock(128)\n        self.se3 = SEBlock(256)\n        self.se4 = SEBlock(512)\n\n        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n        self.d3 = ConvBlock(512 + 256, 256)\n        self.d2 = ConvBlock(256 + 128, 128)\n        self.d1 = ConvBlock(128 + 64,   64)\n        self.d0 = ConvBlock(64  + 64,   64)\n        self.out = nn.Conv2d(64, 1, 1)\n\n    def forward(self, x):\n        a = self.e0(x)     \n        b = self.e1(a)     \n        b = self.se1(b)\n        c = self.e2(b)     \n        c = self.se2(c)\n        d = self.e3(c)     \n        d = self.se3(d)\n        e = self.e4(d)     \n        e = self.se4(e)\n\n        x = self.d3(torch.cat([self.up(e), d], 1))\n        x = self.d2(torch.cat([self.up(x), c], 1))\n        x = self.d1(torch.cat([self.up(x), b], 1))\n        x = self.d0(torch.cat([self.up(x), a], 1))\n        x = self.up(x)\n        return self.out(x)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.899241Z","iopub.execute_input":"2025-12-31T12:27:03.899542Z","iopub.status.idle":"2025-12-31T12:27:03.912720Z","shell.execute_reply.started":"2025-12-31T12:27:03.899517Z","shell.execute_reply":"2025-12-31T12:27:03.912015Z"}},"outputs":[],"execution_count":5},{"id":"e2b64cd7-059f-4491-9c70-89206f759847","cell_type":"code","source":"\ndef DeepLabV3_R50():\n    m = segmodels.deeplabv3_resnet50(weights=\"DEFAULT\")\n    m.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.913681Z","iopub.execute_input":"2025-12-31T12:27:03.914042Z","iopub.status.idle":"2025-12-31T12:27:03.927506Z","shell.execute_reply.started":"2025-12-31T12:27:03.914017Z","shell.execute_reply":"2025-12-31T12:27:03.926817Z"}},"outputs":[],"execution_count":6},{"id":"d895a4f5-b96e-4d6a-bb9b-4e9dd857beed","cell_type":"code","source":"\ndef FCN_R50():\n    m = segmodels.fcn_resnet50(weights=\"DEFAULT\")\n    m.classifier[4] = nn.Conv2d(512, 1, kernel_size=1)\n    return m\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.928387Z","iopub.execute_input":"2025-12-31T12:27:03.928667Z","iopub.status.idle":"2025-12-31T12:27:03.938584Z","shell.execute_reply.started":"2025-12-31T12:27:03.928638Z","shell.execute_reply":"2025-12-31T12:27:03.937870Z"}},"outputs":[],"execution_count":7},{"id":"da2bf38e-b0d8-490f-8d24-39d230ef8154","cell_type":"code","source":"\nclass SegNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n        self.enc = nn.ModuleList([\n            vgg[:5], vgg[5:10], vgg[10:17], vgg[17:24], vgg[24:31]\n        ])\n        self.dec = nn.ModuleList([\n            nn.ConvTranspose2d(512,512,2,2),\n            nn.ConvTranspose2d(512,256,2,2),\n            nn.ConvTranspose2d(256,128,2,2),\n            nn.ConvTranspose2d(128,64,2,2),\n            nn.ConvTranspose2d(64,1,2,2),\n        ])\n\n    def forward(self, x):\n        for e in self.enc:\n            x = e(x)\n            x = F.max_pool2d(x, 2)\n        for d in self.dec:\n            x = d(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.939530Z","iopub.execute_input":"2025-12-31T12:27:03.939808Z","iopub.status.idle":"2025-12-31T12:27:03.949121Z","shell.execute_reply.started":"2025-12-31T12:27:03.939777Z","shell.execute_reply":"2025-12-31T12:27:03.948432Z"}},"outputs":[],"execution_count":8},{"id":"1d15df41-3158-4169-9b65-7a14e20c3cce","cell_type":"markdown","source":"# MODEL FACTORY + CACHE","metadata":{}},{"id":"1c7a2c83-37e6-471f-b490-2a21218d3236","cell_type":"code","source":"MODEL_BUILDERS = {\n    \"VGG16_UNet\": VGG16_UNet,\n    \"DeepLabV3_R50\": DeepLabV3_R50,\n    \"FCN_R50\": FCN_R50,\n    \"ResNet34_SE_UNet\": ResNet34_SE_UNet,\n    \"SegNet\": SegNet,\n}\n\n_loaded_models = {}\n\ndef get_model(arch_name):\n    \n    if arch_name in _loaded_models:\n        return _loaded_models[arch_name]\n\n    model = MODEL_BUILDERS[arch_name]()\n    model = load_state_safely(model, WEIGHTS[arch_name])\n    model = model.to(DEVICE).eval()\n    _loaded_models[arch_name] = model\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.949898Z","iopub.execute_input":"2025-12-31T12:27:03.950170Z","iopub.status.idle":"2025-12-31T12:27:03.961224Z","shell.execute_reply.started":"2025-12-31T12:27:03.950135Z","shell.execute_reply":"2025-12-31T12:27:03.960598Z"}},"outputs":[],"execution_count":9},{"id":"d2790df5-d49a-4e27-b83b-943f844cfc08","cell_type":"markdown","source":"# INFERENCE FUNCTION (for GUI)","metadata":{}},{"id":"b1a54fd6-6819-4cd0-9177-3bf9f9beb5d5","cell_type":"code","source":"@torch.no_grad()\ndef predict(image, arch_name, thresh):\n    if image is None:\n        return None, None, \"No image provided.\"\n\n\n    rgb = np.array(image).astype(np.uint8)\n    h, w = rgb.shape[:2]\n\n    model = get_model(arch_name)\n\n    x = preprocess_image(rgb, size=IMG_SIZE).to(DEVICE)\n    logits = model_forward(model, x, arch_name)\n\n    mask = postprocess_mask(logits, out_h=h, out_w=w, thresh=float(thresh))\n    over = overlay_mask(rgb, mask, alpha=0.45)\n\n    info = f\"Device: {DEVICE} | Model: {arch_name} | Threshold: {thresh}\"\n    return mask, over, info\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.964349Z","iopub.execute_input":"2025-12-31T12:27:03.964574Z","iopub.status.idle":"2025-12-31T12:27:03.973458Z","shell.execute_reply.started":"2025-12-31T12:27:03.964551Z","shell.execute_reply":"2025-12-31T12:27:03.972692Z"}},"outputs":[],"execution_count":10},{"id":"4f51ea44-45cd-488c-bc91-91311f79e9c5","cell_type":"markdown","source":"# GRADIO UI","metadata":{}},{"id":"ee229fd8-8de8-4023-99d6-1eb08d615a20","cell_type":"code","source":"with gr.Blocks(title=\"ISIC Segmentation GUI\") as demo:\n    gr.Markdown(\"## Skin Lesion Segmentation (Upload Image → Choose Model → Get Mask)\")\n\n    with gr.Row():\n        inp = gr.Image(type=\"numpy\", label=\"Upload Image (RGB)\")\n        with gr.Column():\n            model_dd = gr.Dropdown(\n                choices=list(MODEL_BUILDERS.keys()),\n                value=\"VGG16_UNet\",\n                label=\"Choose Model\"\n            )\n            thr = gr.Slider(0.1, 0.9, value=THRESH, step=0.05, label=\"Mask Threshold\")\n            btn = gr.Button(\"Run Inference\")\n\n    with gr.Row():\n        out_mask = gr.Image(type=\"numpy\", label=\"Predicted Mask (binary)\")\n        out_overlay = gr.Image(type=\"numpy\", label=\"Overlay (mask on image)\")\n\n    status = gr.Textbox(label=\"Status\", interactive=False)\n\n    btn.click(\n        fn=predict,\n        inputs=[inp, model_dd, thr],\n        outputs=[out_mask, out_overlay, status]\n    )\n\n    gr.Markdown(\n        \"### Notes\\n\"\n        \"- Make sure your weight files exist at the paths in the `WEIGHTS` dictionary.\\n\"\n        \"- If you trained on different normalization/size, match those in `preprocess_image()`.\\n\"\n    )\n\ndemo.launch(debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:27:03.974376Z","iopub.execute_input":"2025-12-31T12:27:03.974895Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://78db607043283e1579.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://78db607043283e1579.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 83.3M/83.3M [00:00<00:00, 170MB/s] \n","output_type":"stream"}],"execution_count":null},{"id":"fc18576c-c34c-49a3-8b70-7b04aaf57a3b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}